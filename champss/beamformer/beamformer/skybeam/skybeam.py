import attr
from attr.validators import instance_of
import numpy as np
import logging

from sps_common.conversion import (
    write_to_filterbank,
    convert_ra_dec,
    unix_to_mjd,
    subband,
    read_intensity_hdf5,
    fill_and_norm,
)
from sps_common.filterbank import get_dtype
from sps_common import constants

from beamformer.utilities.blurmask import *

LOGGING_CONFIG = {}
logging_format = "[%(asctime)s] %(levelname)s::%(module)s: %(message)s"
logging.basicConfig(format=logging_format, level=logging.DEBUG)
logger = logging.getLogger()

_tsamp = constants.TSAMP

@attr.s(slots=True)
class SkyBeam(object):
    """
    SkyBeam class to create the sky beam from intensity data.

    Parameters
    =======
    ra: float
        Right Ascension of the beam in degrees

    dec: float
        Declination of the beam in degrees

    nchan: int
        Number of channels in the beam

    length: int
        Length of skybeam in number of time samples

    maxdm: float
        The max DM to dedisperse the data to

    max_beams: list
        The list of beams to append to spectra and their start and end time in unix UTC time

    beam_row: int
        FRB beam row number of the sky beam

    tsamp: float
        Length of time samples in seconds (Default = 0.00098304 s)

    data_list: list
        List of all the msgpack/hdf5 files required for this sky beam

    nbits: int
        Number of bits in the beamformer (Default = 32 bits)
    """

    ra = attr.ib(default=None, validator=instance_of(float))
    dec = attr.ib(default=None, validator=instance_of(float))
    nchans = attr.ib(default=None, validator=instance_of(int))
    length = attr.ib(default=None, validator=instance_of(int))
    maxdm = attr.ib(default=None, validator=instance_of(float))
    max_beams = attr.ib(default=None, validator=instance_of(list))
    beam_row = attr.ib(default=None, validator=instance_of(int))
    tsamp = attr.ib(default=_tsamp, validator=instance_of(float))
    data_list = attr.ib(default=None, validator=instance_of(list))
    nbits = attr.ib(default=32, validator=instance_of(int))
    _rfi_mask = attr.ib(init=False)
    utc_start = attr.ib(init=False)
    spectra = attr.ib(init=False)
    processed = attr.ib(init=False)
    birdies = attr.ib(init=False)

    @ra.validator
    def _validate_ra(self, attribute, value):
        assert 0.0 <= value <= 360.0, "RA must be between 0 and 360"

    @dec.validator
    def _validate_dec(self, attribute, value):
        assert -90.0 <= value <= 90.0, "Dec must be between -90 and 90"

    @nchans.validator
    def _validate_nchans(self, attribute, value):
        assert value in [
            1024,
            2048,
            4096,
            8192,
            16384,
        ], "nchans must be either 1024, 2048, 4096, 8192 or 16384"

    @length.validator
    def _validate_ntime(self, attribute, value):
        assert value > 0, "length of pointing must be larger than zero"

    @beam_row.validator
    def _validate_beam_row(self, attribute, value):
        assert 0 <= value <= 255, "beam row must be between 0 and 255"

    def __attrs_post_init__(self):
        dtype = get_dtype(self.nbits)
        self.utc_start = self.max_beams[0]["utc_start"]
        self.spectra = np.ma.zeros((self.nchans, self.length), dtype=dtype)
        self._rfi_mask = np.zeros(self.spectra.shape, dtype=bool)
        self.processed = [False] * self.length
        self.birdies = []

    @classmethod
    def from_active_pointing_dict(cls, pointing_dict, nbits=32):
        """
        Load a dictionary of a pointing generated by PointingStrategist

        Parameters
        =======
        pointing_dict: dict
            A dictionary of properties of an active pointing generated by PointingStrategist

        nbits: int
            Number of bits in the RFI-cleaned hdf5 files
        """
        attr_list = [
            "ra",
            "dec",
            "nchans",
            "length",
            "maxdm",
            "max_beams",
            "beam_row",
            "data_list",
        ]
        pointing_dict = {k: v for k, v in pointing_dict.items() if k in attr_list}
        pointing = cls(**pointing_dict)
        return cls(
            ra=pointing.ra,
            dec=pointing.dec,
            nchans=pointing.nchans,
            length=pointing.length,
            maxdm=pointing.maxdm,
            max_beams=pointing.max_beams,
            beam_row=pointing.beam_row,
            data_list=pointing.data_list,
            nbits=nbits,
        )

    def is_complete(self):
        """
        Check if the spectra is completely filled
        """
        return False not in self.processed

    def dedisperse(self):
        """
        Dedisperse the data and return the dedispersed time series
        """
        pass

    def get_mask(self):
        """
        Returns the RFI mask of the spectra

        Returns
        =======
        self.spectra.mask: np.ndarray
            An np.ndarray of boolean mask of the spectra
        """
        # make sure rfi mask is copied from spectra
        if np.ma.isMaskedArray(self.spectra):
            self._rfi_mask = self.spectra.mask
        return self._rfi_mask

    def update_mask(self):
        """
        Updates the RFI mask of the spectra with rfi_mask
        """
        # make sure rfi mask is copied from spectra
        if np.ma.isMaskedArray(self.spectra):
            self.spectra.mask = self._rfi_mask

    def adjust_mask(
        self,
        chan_flag_frac,
        sub_flag_frac,
        do_clumping=False,
        clump_window=3,
        clump_thresh=5,
    ):
        """ Find areas of rfi_mask which are mostly flagged, and flag that whole area.
        It tries to find vertical lines, horizontal lines, and clumps.

        Lines are found by assessing if the fraction of masked data along that line is
        over the flag fractions.

        Clumps are found by removing flags from existing lines, blurring the remaining
        mask by summing each element with its neighbours, masking elements if the clumped
        values are greater than a threshold.
        
        Assumes nchan x nsubint mask - aka chan_flag_frac acts on axis=0

        Parameters
        =======
        chan_flag_frac: float
            If > chan_flag_frac of a channel has been flagged, flag whole channel

        sub_flag_frac: float
            If > sub_flag_frac of a subint has been flagged, flag whole subint

        do_clumping: bool
            Whether to look for clumps of flagged elements in the mask

        clump_window: int
            Size of blurring window to use if do_clumping.
            Must be an odd integer.
            A clump_window x clump_window box around each element will be summed

        clump_thresh: float
            Threshold for masking clumps. If found > clump_thresh flags in the
            window around an element, flag that element.
        """
        if do_clumping:
            if type(clump_window) != int or not (clump_window % 2):
                raise TypeError("clump_window must be an odd integer")
            if clump_thresh < 0 or clump_thresh > clump_window ** 2:
                raise ValueError(
                    "clump_thresh must be between 0 and the number \
                                  of elements in the blurring window"
                )
        if sub_flag_frac <= 0 or sub_flag_frac > 1:
            raise ValueError("sub_flag_frac must be between 0 and 1")
        if chan_flag_frac <= 0 or chan_flag_frac > 1:
            raise ValueError("chan_flag_frac must be between 0 and 1")

        logger.info("Original fraction of data masked: {0}".format(np.mean(self.spectra.mask)))
            
        out_array = np.full(self.spectra.mask.shape, False)

        logger.info("Thresholding channels and intervals")
        chan_fracs = np.mean(self.spectra.mask, axis = 1)
        subint_fracs = np.mean(self.spectra.mask, axis = 0)
        
        for chan in range(self.spectra.mask.shape[0]):
            if chan_fracs[chan] >= chan_flag_frac and chan_fracs[chan] < 1:
                out_array[chan, :] = True
        for subint in range(self.spectra.mask.shape[1]):
            if subint_fracs[subint] >= sub_flag_frac and subint_fracs[subint] < 1:
                out_array[:, subint] = True
                
        logger.info("Fraction of data masked from subint and channel thresholds: {0}".format(np.mean(out_array)))
                     
        if do_clumping:
            logger.info("Performing clumping analysis")
            if (
                clump_window == 3
            ):  # blur_3x3 is theoretically faster, but have not timed to see(!)
                clumps = (
                    blur_3x3(
                        np.logical_and(self.spectra.mask, np.logical_not(out_array)),
                        mean=False,
                    )
                    > clump_thresh
                )
            else:
                clumps = (
                    blur(
                        np.logical_and(self.spectra.mask, np.logical_not(out_array)),
                        mean=False,
                        window=clump_window,
                    )
                    > clump_thresh
                )
            logger.info("Fraction of data masked from clumps: {0}".format(np.mean(clumps)))
            logger.info("Total fraction of data masked: {0}".format(np.mean(clumps + out_array + self.spectra.mask)))
            self._rfi_mask = clumps + out_array + self.spectra.mask > 0
        else:
            logger.info("Total fraction of data masked: {0}".format(np.mean(out_array + self.spectra.mask)))
            self._rfi_mask = out_array + self.spectra.mask > 0

        logger.info("Updating Mask")
        self.update_mask()

    def process_storage_hdf5(self):
        """
        Load intensity data from storage and append to spectra
        """
        if not self.data_list:
            raise Exception("Data list is empty")
        if not self.data_list[0].endswith(".hdf5"):
            raise Exception("File type is not hdf5")
        for datapath in self.data_list:
            data, mask, metadata = read_intensity_hdf5(datapath)
            beam_no, start, end = (
                metadata["beam_number"],
                metadata["start"],
                metadata["end"],
            )
            for beam in self.max_beams:
                if beam["beam"] == beam_no:
                    beam_start_end = beam
            intensity = np.ma.array(data, mask=mask)
            if end < beam_start_end["utc_start"]:
                print("{} is outside of transit. Skipping...".format(datapath))
                continue
            if start < beam_start_end["utc_start"]:
                start_chunk = int(
                    round((beam_start_end["utc_start"] - start) / self.tsamp)
                )
                start += start_chunk * self.tsamp
                intensity = intensity[:, start_chunk:]
            if end > beam_start_end["utc_end"]:
                from_end_chunk = int(
                    round((end - beam_start_end["utc_end"]) / self.tsamp)
                )
                end -= from_end_chunk * self.tsamp
                intensity = intensity[:, :-from_end_chunk]
            print("Adding {} to skybeam".format(datapath))
            self.append_intensity(intensity, start, end)
            if self.is_complete():
                print("beamforming completed")
                self._rfi_mask = self.spectra.mask
                break

    def append_intensity(self, intensity, intensity_utc_start, intensity_utc_end=None):
        """
        Append the beamformer spectra with the intensity data from the FRB beam with a start time
        for the intensity data chunk

        Parameters
        =======
        intensity: ndarray
            the 2-D intensity data chunk

        intensity_utc_start: float
            the start time of the intensity chunk as unix UTC time

        intensity_utc_end: float
            the end time of the intensity chunk as unix UTC time
        """
        if intensity.shape[0] > self.nchans:
            intensity = subband(intensity, self.nchans)
        time_diff = intensity_utc_start - self.utc_start
        if intensity_utc_end is None:
            intensity_length = intensity.shape[1]
        else:
            intensity_length = int(
                round((intensity_utc_end - intensity_utc_start) / self.tsamp)
            )
        ichunk = int(round(time_diff / self.tsamp))
        if ichunk < 0:
            self.spectra[:, 0 : ichunk + intensity_length] = intensity[
                :, -ichunk:intensity_length
            ]
            self.processed[0 : ichunk + intensity_length] = [True] * (
                ichunk + intensity_length
            )
        elif ichunk + intensity_length > self.length:
            self.spectra[:, ichunk:] = intensity[:, : (self.length - ichunk)]
            self.processed[ichunk:] = [True] * (self.length - ichunk)
        else:
            self.spectra[:, ichunk : ichunk + intensity_length] = intensity[
                :, :intensity_length
            ]
            self.processed[ichunk : ichunk + intensity_length] = [
                True
            ] * intensity.shape[1]

    def get_birdies(self):
        """
        Get the list of birdies from the bird files

        Returns
        =======
        birdies: list
            A list of birdies from the accompanying birds file from RFI mitigation
        """
        self.birdies = []
        if not self.data_list:
            raise Exception("Data list is empty")
        for datapath in self.data_list:
            birds_path = datapath.rstrip(".hdf5") + ".birds"
            try:
                with open(birds_path, "r") as birds_file:
                    for birds in birds_file.readlines():
                        if not birds.startswith("#"):
                            self.birdies.append(birds)
            except IOError as e:
                print("Error opening {}".format(birds_path))
                print(e)

        self.birdies = sorted(self.birdies)
        return self.birdies

    def mask_spectra(self):
        """
        Mask the spectra with a median normalisation process
        """
        if np.ma.isMaskedArray(self.spectra):
            self.spectra = fill_and_norm(
                self.spectra, self.spectra.shape[0], self.spectra.shape[1], 4096
            )

    def write_spectra(self, oname, mode="filterbank"):
        """
        Write spectra to different formats. Currently only filterbank is available

        Parameters
        =======
        mode: str
            The mode to save the spectra. Default is filterbank mode
        """
        if mode == "filterbank":
            # make sure spectra is masked before writing
            self.mask_spectra()
            srcra, srcdec = convert_ra_dec(self.ra, self.dec)
            if srcdec.startswith("-"):
                srcname = "J{0}{1}".format(srcra[:4], srcdec[:3])
            else:
                srcname = "J{0}+{1}".format(srcra[:4], srcdec[:2])
            start_mjd = unix_to_mjd(self.utc_start)
            write_to_filterbank(
                self.spectra,
                self.nchans,
                self.length,
                self.beam_row,
                start_mjd,
                self.nbits,
                srcname,
                srcra,
                srcdec,
                oname,
            )
        if self.birdies:
            birds_filename = oname.rstrip(".fil") + ".birds"
            with open(birds_filename, "w") as outfile:
                for birds in self.birdies:
                    outfile.write("{}\n".format(birds))
